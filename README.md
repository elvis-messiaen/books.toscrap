# Books to Scrape - Projet Complet Scrapy + FastAPI + PostgreSQL

Projet complet combinant scraping de donn√©es avec Scrapy, base de donn√©es PostgreSQL et exposition via une API REST avec FastAPI. Ce projet extrait des donn√©es du site books.toscrape.com, les stocke en base PostgreSQL et les rend accessibles via une architecture moderne en couches.

## Architecture du Projet

### Structure Clean Architecture
```
‚îú‚îÄ‚îÄ main.py                     # Point d'entr√©e de l'API FastAPI
‚îú‚îÄ‚îÄ api/                        # Architecture en couches
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Mod√®les de donn√©es (Book, Category)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book.py            # Dataclass Book avec validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ category.py        # Mod√®le Category
‚îÇ   ‚îú‚îÄ‚îÄ interfaces/             # Contrats d'interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_repository_interface.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_service_interface.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categories_repository_interface.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ categories_service_interface.py
‚îÇ   ‚îú‚îÄ‚îÄ repositories/           # Acc√®s aux donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_repository.py  # Repository livres (JSON ‚Üí Book)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ category_repository.py # Repository cat√©gories
‚îÇ   ‚îú‚îÄ‚îÄ services/              # Logique m√©tier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_service.py    # Service livres avec validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ category_service.py # Service cat√©gories
‚îÇ   ‚îî‚îÄ‚îÄ routes/                # Routes FastAPI
‚îÇ       ‚îú‚îÄ‚îÄ books.py           # Endpoints livres (/books/*)
‚îÇ       ‚îî‚îÄ‚îÄ categories.py      # Endpoints cat√©gories (/categories/*)
‚îú‚îÄ‚îÄ books_toscrape/            # Projet Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ spiders/               # Spiders de scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ books_spider.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categories_spider.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ books_by_categories_spider.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ details_book_spider.py
‚îÇ   ‚îú‚îÄ‚îÄ categories.json        # Donn√©es extraites
‚îÇ   ‚îú‚îÄ‚îÄ books_by_categories.json
‚îÇ   ‚îî‚îÄ‚îÄ detail_books.json
‚îî‚îÄ‚îÄ requirements.txt           # D√©pendances (Scrapy + FastAPI)
```

## Installation et Configuration

### Pr√©requis
- Python 3.8+
- pip (gestionnaire de paquets Python)

### Installation des d√©pendances
```bash
# Installer toutes les d√©pendances n√©cessaires (Scrapy + FastAPI)
pip install -r requirements.txt
```

### D√©pendances incluses
- `scrapy` - Framework de scraping
- `fastapi` - Framework API moderne
- `uvicorn[standard]` - Serveur ASGI haute performance
- `python-multipart` - Support des formulaires
- `pydantic` - Validation de donn√©es

## Partie Scrapy - Extraction de Donn√©es

### Spiders Disponibles

#### 1. Spider des Cat√©gories
```bash
# Extraire toutes les cat√©gories du site
cd books_toscrape
scrapy crawl categories_spider -o categories.json
```
**Donn√©es extraites :** nom, url, url_relative

#### 2. Spider des Livres par Cat√©gorie
```bash
# Extraire tous les livres organis√©s par cat√©gorie
cd books_toscrape
scrapy crawl books_by_categories_spider -o books_by_categories.json
```
**Donn√©es extraites :** category, title, price, star_rating, image_url, url, availability

#### 3. Spider des D√©tails Complets
```bash
# Extraire les d√©tails complets de chaque livre (processus long)
cd books_toscrape
scrapy crawl details_book_spider
```
**Donn√©es extraites :** 20+ champs incluant description, code UPC, taxes, livres recommand√©s, etc.

### Processus Complet de Scraping
```bash
# Commande compl√®te pour extraire toutes les donn√©es
cd books_toscrape
scrapy crawl categories_spider -o categories.json && \
scrapy crawl books_by_categories_spider -o books_by_categories.json && \
scrapy crawl details_book_spider
```

### Formats d'Export Disponibles
```bash
# JSON (recommand√© pour FastAPI)
scrapy crawl categories_spider -o categories.json

# CSV (pour analyse Excel)
scrapy crawl categories_spider -o categories.csv

# XML (format structur√©)
scrapy crawl categories_spider -o categories.xml
```

## Partie FastAPI - API REST

### Architecture Clean Code
L'API suit les principes de Clean Architecture avec s√©paration claire des responsabilit√©s :

- **Routes** : Gestion des requ√™tes HTTP, validation des entr√©es
- **Services** : Logique m√©tier, validation des donn√©es
- **Repositories** : Acc√®s aux donn√©es PostgreSQL et JSON
- **Models** : Structures de donn√©es typ√©es avec Pydantic

### Lancement de l'API

#### D√©veloppement
```bash
# Lancer l'API FastAPI directement
python main.py
# Accessible sur http://localhost:8000
```

#### Alternative avec uvicorn
```bash
# Serveur de d√©veloppement avec rechargement automatique
uvicorn main:app --reload
# Accessible sur http://localhost:8000
```

#### Production
```bash
# Serveur optimis√© pour la production
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Endpoints API Disponibles

#### Documentation
- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc
- **Schema OpenAPI** : http://localhost:8000/openapi.json

#### Endpoints Livres
```bash
# R√©cup√©rer tous les livres
GET /books/

# R√©cup√©rer un livre par ID
GET /books/{id}

# Livres par cat√©gorie
GET /books/category/{category_name}

# Recherche avanc√©e avec filtres
GET /books/search/?titre=harry&prix_min=10&prix_max=50&note_min=4
```

#### Endpoints Cat√©gories
```bash
# R√©cup√©rer toutes les cat√©gories
GET /categories/

# R√©cup√©rer une cat√©gorie par ID
GET /categories/{id}

# Prix moyen des livres par cat√©gorie
GET /categories/prix-moyen/

# Top des cat√©gories par nombre de livres
GET /categories/top-nombre-livres/
```

### Exemples d'Utilisation de l'API

#### Avec cURL
```bash
# R√©cup√©rer tous les livres
curl http://localhost:8000/books/

# R√©cup√©rer un livre sp√©cifique
curl http://localhost:8000/books/1

# Recherche par cat√©gorie
curl http://localhost:8000/books/category/Mystery

# Recherche avec filtres multiples
curl "http://localhost:8000/books/search/?titre=sharp&prix_min=20&prix_max=60"

# R√©cup√©rer toutes les cat√©gories
curl http://localhost:8000/categories/

# Prix moyen par cat√©gorie
curl http://localhost:8000/categories/prix-moyen/

# Top cat√©gories par nombre de livres
curl http://localhost:8000/categories/top-nombre-livres/
```

#### Avec Python (requests)
```python
import requests

# R√©cup√©rer tous les livres
response = requests.get("http://localhost:8000/books/")
livres = response.json()

# Recherche avec filtres
params = {"titre": "potter", "prix_max": 30}
response = requests.get("http://localhost:8000/books/search/", params=params)
resultats = response.json()

# Statistiques des cat√©gories
response = requests.get("http://localhost:8000/categories/prix-moyen/")
prix_moyens = response.json()

response = requests.get("http://localhost:8000/categories/top-nombre-livres/")
top_categories = response.json()
```

## Fonctionnalit√©s Avanc√©es

### Validation Automatique
- **Pydantic Models** : Validation automatique des types de donn√©es
- **Gestion d'erreurs** : Codes HTTP appropri√©s (400, 404, 500)
- **Documentation automatique** : Sch√©mas g√©n√©r√©s automatiquement

### Conversion Intelligente
- **JSON ‚Üí Book** : Conversion automatique des donn√©es Scrapy vers objets typ√©s
- **Compatibilit√©** : Gestion des diff√©rents noms de champs entre spiders
- **Valeurs par d√©faut** : Gestion gracieuse des champs manquants

### Performance
- **Architecture modulaire** : Services r√©utilisables et testables
- **S√©paration des couches** : Repository pattern pour l'acc√®s aux donn√©es
- **Validation m√©tier** : Logique centralis√©e dans les services

## üõ†Ô∏è D√©veloppement et Tests

### Tests de Syntaxe
```bash
# V√©rifier la syntaxe d'un spider
python -m py_compile books_toscrape/books_toscrape/spiders/books_spider.py

# Lister tous les spiders disponibles
cd books_toscrape
scrapy list
```

### D√©bogage avec Scrapy Shell
```bash
# Tester les s√©lecteurs interactivement
cd books_toscrape
scrapy shell "https://books.toscrape.com/index.html"
```

```python
# Dans le shell Scrapy
response.css('div.side_categories ul li ul li a::text').getall()
len(response.css('article.product_pod'))
```

### Test de l'API
```bash
# Tester la sant√© de l'API
curl http://localhost:8000/health

# Voir la page d'accueil avec informations
curl http://localhost:8000/
```

## Performance et Optimisation

### Scrapy
- **Spider optimis√©** : R√©utilisation des donn√©es entre spiders
- **Pagination automatique** : Parcours complet sans intervention
- **Gestion d'erreurs** : Reprise automatique en cas d'√©chec

### FastAPI
- **Serveur ASGI** : Uvicorn pour les performances maximales
- **Validation Pydantic** : Validation rapide des donn√©es
- **Documentation automatique** : Pas de maintenance suppl√©mentaire

## Cas d'Usage

### Analyse de Donn√©es
```bash
# Extraire toutes les donn√©es pour analyse
cd books_toscrape
scrapy crawl details_book_spider

# Lancer l'API pour l'exploration interactive
python main.py
# Utiliser http://localhost:8000/docs pour explorer
```

### Int√©gration Frontend
```javascript
// R√©cup√©rer des livres pour une interface utilisateur
fetch('http://localhost:8000/books/')
  .then(response => response.json())
  .then(books => {
    // Afficher les livres dans l'interface
    books.forEach(book => console.log(book.title));
  });
```

### Export et Analyse
```bash
# Export CSV pour Excel
cd books_toscrape
scrapy crawl books_by_categories_spider -o livres_analyse.csv

# Puis utiliser l'API pour des requ√™tes sp√©cifiques
curl "http://localhost:8000/books/search/?prix_max=25" | jq '.'
```

## Migration Future : PostgreSQL

Cette architecture est con√ßue pour une migration facile vers PostgreSQL :

1. **Models Book** : Mapping direct vers tables SQL
2. **Repository Pattern** : Remplacement JSON ‚Üí PostgreSQL transparent
3. **Services inchang√©s** : Logique m√©tier pr√©serv√©e
4. **Clean Architecture** : Migration par couches sans impact sur l'API

L'utilisation de dataclass `Book` au lieu de `dict` pr√©pare parfaitement cette √©volution !

## Notes Importantes

- **Temps d'ex√©cution** : Le spider des d√©tails peut prendre 15-30 minutes (1000+ pages)
- **Ressources** : Le scraping complet g√©n√®re plusieurs MB de donn√©es JSON
- **D√©veloppement** : Utilisez `--reload` uniquement en d√©veloppement
- **Production** : Configurez des workers multiples pour la performance